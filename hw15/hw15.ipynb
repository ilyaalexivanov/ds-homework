{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/pytorch/examples/tree/master/word_language_model \n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words: \n",
    "                    self.dictionary.add_word(word)  \n",
    "        \n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "num_epochs = 3\n",
    "num_samples = 500\n",
    "batch_size = 20\n",
    "seq_length = 30\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus()\n",
    "#ids = corpus.get_data('anna.txt', batch_size)\n",
    "ids = corpus.get_data('reviews.txt', batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        x = self.embed(x)\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        out = self.linear(out)\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(states):\n",
    "    return [state.detach() for state in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step[0/324], Loss: 10.0585, Perplexity: 23353.19\n",
      "Epoch [1/3], Step[100/324], Loss: 7.2074, Perplexity: 1349.44\n",
      "Epoch [1/3], Step[200/324], Loss: 6.4695, Perplexity: 645.14\n",
      "Epoch [1/3], Step[300/324], Loss: 6.8140, Perplexity: 910.52\n",
      "Epoch [2/3], Step[0/324], Loss: 6.1222, Perplexity: 455.87\n",
      "Epoch [2/3], Step[100/324], Loss: 5.7712, Perplexity: 320.93\n",
      "Epoch [2/3], Step[200/324], Loss: 5.1787, Perplexity: 177.45\n",
      "Epoch [2/3], Step[300/324], Loss: 5.3867, Perplexity: 218.49\n",
      "Epoch [3/3], Step[0/324], Loss: 4.9553, Perplexity: 141.93\n",
      "Epoch [3/3], Step[100/324], Loss: 4.6535, Perplexity: 104.95\n",
      "Epoch [3/3], Step[200/324], Loss: 4.0111, Perplexity: 55.21\n",
      "Epoch [3/3], Step[300/324], Loss: 3.8243, Perplexity: 45.80\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    states = ( torch.zeros(num_layers, batch_size, hidden_size),\n",
    "                    torch.zeros(num_layers, batch_size, hidden_size))\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length]\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length]\n",
    "        # Forward pass\n",
    "        states = detach(states)\n",
    "        outputs, states = model(inputs, states)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model1.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First word is random word, next ones are predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flavors to it spa with a side of mine! and that its Chinese! Service was Chinese, (Thanks Owen!), when it is beter than out, shoping, you may have skewed The decor is rare and their cheseburger.. sauce were oh-so-god. special, round. Seriously, I highly recomend Whitey I highly recomend lim√≥n amarilo and have order to ben Tony packed, very big and lots of staf. charge for the crust. Ad the Nordstrom to the top notch Another there are many more satisfying. That from traditional So you can pul Touch of the fod. knowledgeable a sample extravaganza and then the waiter subsequently the wonderful things of host for my years. The 18 is wel Granted, and with the stay Their bers are great. \\nMy husband and I recently I could eat here (198) and once al true. talking, I wil start this place alot. Yelp! months Shoe \\nTried $ Pan casual folks, lengthwise, Turkish National of membership for me. Again, absolute delight in was our first impresion did not se what it was the same time. Nothing though, You may ned there, people note and fixing every line, made me be blown advice back to Old Town else is. This is a suny and top serving. They also lots of the shel which is unheard I do not ned to ad up to it, I think that lose shoes the litle re: is that god. Not only every by months. Canyon they cal her Let Whitey perfectly. priced, save fries are? first year of Lewis I have to stay about them, close. amarilo god! is, sliced bufet, For fit, I highly recomend them \"simple\" stay \\nBrokstone tea at Benihana, formation Scot like 80% were fresh and it and just remember me in wearing drinker & tomato, Trust the orange two salad with Kazimierz. raspbery homemade Israeli cucumbers It is les but the story goes I have tried their soups which was nice to go to; gasping, laughing, held each item, are not stuck and I do not recomend Whitey The store tasted so the meting, was empty, For hot sauce was awesome, and the fod was great just came it in a very rich... folowed I had them I received an regular chicken (which as a swet potato fries. I do not take my hair place. Way After \\nOne Tyler yum. i came in Whitey to town Republic, for a month, to visit right on, another To I have eaten more. I had the fod & for diner flavors it. We arived at home Thank you hubs\\' and soups, on to top it was located on. milk chocolate, pesto focacia He gave Gabi to hear whenever The only ones in was a stuco complete friendly wel $2.50!!! as wel as cucumber. (and made focacia means Let the quality. hours on buy the puled pork flowing and toping came to someone was perfect. A preparation to start at them day Magic Al were the perfect size along to '"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_words = \"\"\n",
    "state = (torch.zeros(num_layers, 1, hidden_size), torch.zeros(num_layers, 1, hidden_size))    \n",
    "prob = torch.ones(vocab_size)\n",
    "input = torch.multinomial(prob, num_samples=1).unsqueeze(1)\n",
    "for i in range(num_samples):\n",
    "    output, state = model(input, state)\n",
    "    prob = output.exp()\n",
    "    word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "    input.fill_(word_id)\n",
    "    word = corpus.dictionary.idx2word[word_id]\n",
    "    word = '\\n' if word == '<eos>' else word + ' '\n",
    "    prediction_words = prediction_words + word\n",
    "\n",
    "prediction_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
