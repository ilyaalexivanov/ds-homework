{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сверточные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Реализовать СНН на MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "FOLDER = 'MNIST_data'\n",
    "\n",
    "if not os.path.exists(FOLDER):\n",
    "    os.mkdir(FOLDER)\n",
    "    \n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# данные\n",
    "train_set = datasets.MNIST(root=FOLDER, train=True, transform=trans, download=True)\n",
    "test_set = datasets.MNIST(root=FOLDER, train=False, transform=trans, download=True)\n",
    "\n",
    "# итераторы\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "sample_x, sample_y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (prelu1): PReLU(num_parameters=1)\n",
      "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (prelu2): PReLU(num_parameters=1)\n",
      "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, (5,5), padding=2)\n",
    "        self.prelu1 = nn.PReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "#        self.bn1 = CustomBatchNorm2d(6)\n",
    "        self.conv2 = nn.Conv2d(6, 16, (5,5))\n",
    "        self.prelu2 = nn.PReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(self.bn1(self.prelu1(self.conv1(x))), (2,2))\n",
    "        x = F.max_pool2d(self.bn2(self.prelu2(self.conv2(x))), (2,2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, 0.1)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = LeNet()\n",
    "print (net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Accuracy train: 95.600%\n",
      "Epoch: 1; Accuracy train: 97.688%\n",
      "Epoch: 2; Accuracy train: 97.870%\n",
      "Epoch: 3; Accuracy train: 98.005%\n",
      "Epoch: 4; Accuracy train: 98.100%\n",
      "Epoch: 5; Accuracy train: 98.247%\n",
      "Epoch: 6; Accuracy train: 98.348%\n",
      "Epoch: 7; Accuracy train: 98.435%\n",
      "Epoch: 8; Accuracy train: 98.488%\n",
      "Epoch: 9; Accuracy train: 98.327%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    # trainning\n",
    "    av_loss = 0.\n",
    "    correct = 0.\n",
    "#    for x, y in tqdm.tqdm(train_loader):\n",
    "    for x, y in train_loader:\n",
    "        # рассчитываем функцию потерь\n",
    "        out = net(x)\n",
    "        loss = criterion(out, y)\n",
    "        # оптимизация параметров\n",
    "        ## первым шагом обнулим градиенты предыдущего шага (важный момент,\n",
    "        ## без этого результаты теряют корректность)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # подсчет статистики за эпоху        \n",
    "        pred = out.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "        av_loss += loss.item()\n",
    "    print('Epoch: {}; Accuracy train: {:.3f}%'.format(epoch, correct / len(train_loader.dataset) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9; Accuracy train: 98.530%\n"
     ]
    }
   ],
   "source": [
    "av_loss = 0.\n",
    "correct = 0.\n",
    "for x, y in test_loader:\n",
    "    # рассчитываем функцию потерь\n",
    "    out = net(x)\n",
    "    loss = criterion(out, y)\n",
    "    # подсчет статистики за эпоху        \n",
    "    pred = out.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "    av_loss += loss.item()\n",
    "print('Epoch: {}; Accuracy train: {:.3f}%'.format(epoch, correct / len(test_loader.dataset) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custrom BatchNorm2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBatchNorm2d(nn.BatchNorm2d):\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = y.view(x.size(1), -1)\n",
    "        mu = y.mean(dim=1)\n",
    "        sigma2 = y.var(dim=1)\n",
    "        if self.training is not True:\n",
    "            y = y - self.running_mean.view(-1, 1)\n",
    "            y = y / (self.running_var.view(-1, 1)**.5 + self.eps)\n",
    "        else:\n",
    "            if self.track_running_stats is True:\n",
    "                with torch.no_grad():\n",
    "                    self.running_mean = (1-self.momentum)*self.running_mean + self.momentum*mu\n",
    "                    self.running_var = (1-self.momentum)*self.running_var + self.momentum*sigma2\n",
    "            y = y - mu.view(-1,1)\n",
    "            y = y / (sigma2.view(-1,1)**.5 + self.eps)\n",
    "\n",
    "        y = self.weight.view(-1, 1) * y + self.bias.view(-1, 1)\n",
    "        return y.view(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet2(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (prelu1): PReLU(num_parameters=1)\n",
      "  (bn1): CustomBatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (prelu2): PReLU(num_parameters=1)\n",
      "  (bn2): CustomBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LeNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, (5,5), padding=2)\n",
    "        self.prelu1 = nn.PReLU()\n",
    "#        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.bn1 = CustomBatchNorm2d(6)\n",
    "        self.conv2 = nn.Conv2d(6, 16, (5,5))\n",
    "        self.prelu2 = nn.PReLU()\n",
    "#        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = CustomBatchNorm2d(16)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(self.bn1(self.prelu1(self.conv1(x))), (2,2))\n",
    "        x = F.max_pool2d(self.bn2(self.prelu2(self.conv2(x))), (2,2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, 0.1)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net2 = LeNet2()\n",
    "print (net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Accuracy train: 9.512%\n",
      "Epoch: 1; Accuracy train: 9.525%\n",
      "Epoch: 2; Accuracy train: 9.497%\n",
      "Epoch: 3; Accuracy train: 9.427%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-750c53d8bae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m## без этого результаты теряют корректность)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# подсчет статистики за эпоху\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/otus-ds/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/otus-ds/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    # trainning\n",
    "    av_loss = 0.\n",
    "    correct = 0.\n",
    "    for x, y in train_loader:\n",
    "        # рассчитываем функцию потерь\n",
    "        out = net2(x)\n",
    "        loss = criterion(out, y)\n",
    "        # оптимизация параметров\n",
    "        ## первым шагом обнулим градиенты предыдущего шага (важный момент,\n",
    "        ## без этого результаты теряют корректность)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # подсчет статистики за эпоху        \n",
    "        pred = out.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "        av_loss += loss.item()\n",
    "    print('Epoch: {}; Accuracy train: {:.3f}%'.format(epoch, correct / len(train_loader.dataset) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_loss = 0.\n",
    "correct = 0.\n",
    "for x, y in test_loader:\n",
    "    # рассчитываем функцию потерь\n",
    "    out = net2(x)\n",
    "    loss = criterion(out, y)\n",
    "    # подсчет статистики за эпоху        \n",
    "    pred = out.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "    av_loss += loss.item()\n",
    "print('Epoch: {}; Accuracy train: {:.3f}%'.format(epoch, correct / len(test_loader.dataset) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer, required\n",
    "import copy\n",
    "\n",
    "class MyRMSProp(Optimizer):\n",
    "    \n",
    "    def __init__(self, params, lr=1e-2, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(MyRMSProp, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(MyRMSProp, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            alpha = group['alpha']\n",
    "            lr = group['lr']\n",
    "            eps = group['eps']\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['grad_squared'] = torch.zeros_like(p.data)\n",
    "\n",
    "                grad_squared = state['grad_squared']\n",
    "\n",
    "                grad_squared.mul_(alpha)\n",
    "                grad_squared.addcmul_(1-alpha, d_p, d_p)\n",
    "\n",
    "                grad_avg = grad_squared.sqrt().add_(eps)\n",
    "\n",
    "                p.data.addcdiv_(-lr, d_p, grad_avg)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, (5,5), padding=2)\n",
    "        self.prelu1 = nn.PReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "#        self.bn1 = CustomBatchNorm2d(6)\n",
    "        self.conv2 = nn.Conv2d(6, 16, (5,5))\n",
    "        self.prelu2 = nn.PReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(self.bn1(self.prelu1(self.conv1(x))), (2,2))\n",
    "        x = F.max_pool2d(self.bn2(self.prelu2(self.conv2(x))), (2,2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, 0.1)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_rmsprop = LeNet()\n",
    "print (net_rmsprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = MyRMSProp(net_rmsprop.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # trainning\n",
    "    av_loss = 0.\n",
    "    correct = 0.\n",
    "    for x, y in train_loader:\n",
    "        # рассчитываем функцию потерь\n",
    "        out = net_rmsprop(x)\n",
    "        loss = criterion(out, y)\n",
    "        # оптимизация параметров\n",
    "        ## первым шагом обнулим градиенты предыдущего шага (важный момент,\n",
    "        ## без этого результаты теряют корректность)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # подсчет статистики за эпоху        \n",
    "        pred = out.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "        av_loss += loss.item()\n",
    "    print('Epoch: {}; Accuracy train: {:.3f}%'.format(epoch, correct / len(train_loader.dataset) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD vs. RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_sgd = LeNet()\n",
    "print (net_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_sgd.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # trainning\n",
    "    av_loss = 0.\n",
    "    correct = 0.\n",
    "    for x, y in train_loader:\n",
    "        # рассчитываем функцию потерь\n",
    "        out = net_sgd(x)\n",
    "        loss = criterion(out, y)\n",
    "        # оптимизация параметров\n",
    "        ## первым шагом обнулим градиенты предыдущего шага (важный момент,\n",
    "        ## без этого результаты теряют корректность)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # подсчет статистики за эпоху        \n",
    "        pred = out.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "        av_loss += loss.item()\n",
    "    print('Epoch: {}; Accuracy train: {:.3f}%'.format(epoch, correct / len(train_loader.dataset) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_loss = 0.\n",
    "correct = 0.\n",
    "for x, y in test_loader:\n",
    "    # рассчитываем функцию потерь\n",
    "    out = net_rmsprop(x)\n",
    "    loss = criterion(out, y)\n",
    "    # подсчет статистики за эпоху        \n",
    "    pred = out.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "    av_loss += loss.item()\n",
    "print('Epoch: {}; Accuracy train: {:.3f}%'.format(epoch, correct / len(test_loader.dataset) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_loss = 0.\n",
    "correct = 0.\n",
    "for x, y in test_loader:\n",
    "    # рассчитываем функцию потерь\n",
    "    out = net_sgd(x)\n",
    "    loss = criterion(out, y)\n",
    "    # подсчет статистики за эпоху        \n",
    "    pred = out.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "    av_loss += loss.item()\n",
    "print('Epoch: {}; Accuracy train: {:.3f}%'.format(epoch, correct / len(test_loader.dataset) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp  Accuracy test: 96.440% \n",
    "### SGD Accuracy test = 98.760%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
